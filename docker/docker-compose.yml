# Usage for Dokploy (Git Provider)
# Deploy by pointing Dokploy to this Git repository.
# Set Build Path to '/docker'.
# Environment variables are managed separately in Dokploy's UI.

name: supabase

services:

  studio:
    container_name: supabase-studio
    image: supabase/studio:2025.04.21-sha-173cc56
    restart: unless-stopped
    networks:
      - dokploy-network # Add to Dokploy network
    # Expose port 3000 internally for Dokploy to route to via service name
    expose:
      - 3000
    healthcheck:
      test:
        [
          "CMD",
          "node",
          "-e",
          "fetch('http://studio:3000/api/platform/profile').then((r) => {if (r.status !== 200) throw new Error(r.status)})"
        ]
      timeout: 10s
      interval: 5s
      retries: 3
    depends_on:
      analytics:
        condition: service_healthy
    environment:
      STUDIO_PG_META_URL: http://meta:8080
      # POSTGRES_PASSWORD should be set via Dokploy Environment variables
      # DEFAULT_ORGANIZATION_NAME, DEFAULT_PROJECT_NAME should be set via Dokploy Environment variables
      # OPENAI_API_KEY should be set via Dokploy Environment variables
      SUPABASE_URL: http://kong:8000
      # SUPABASE_PUBLIC_URL should be set via Dokploy Environment variables
      # SUPABASE_ANON_KEY, SUPABASE_SERVICE_KEY, AUTH_JWT_SECRET should be set via Dokploy Environment variables
      # LOGFLARE_API_KEY should be set via Dokploy Environment variables
      LOGFLARE_URL: http://analytics:4000
      NEXT_PUBLIC_ENABLE_LOGS: true
      NEXT_ANALYTICS_BACKEND_PROVIDER: postgres

  kong:
    container_name: supabase-kong
    image: kong:2.8.1
    restart: unless-stopped
    networks:
      - dokploy-network # Add to Dokploy network
    # Map host ports as in your old Dokploy compose and the original official compose
    ports:
      - ${KONG_HTTP_PORT}:8000/tcp
      - ${KONG_HTTPS_PORT}:8443/tcp
    volumes:
      # Relative path './volumes' is correct as it's relative to this compose file's location (/docker)
      - ./volumes/api/kong.yml:/home/kong/temp.yml:ro,z
    depends_on:
      analytics:
        condition: service_healthy
    environment:
      KONG_DATABASE: "off"
      KONG_DECLARATIVE_CONFIG: /home/kong/kong.yml
      KONG_DNS_ORDER: LAST,A,CNAME
      KONG_PLUGINS: request-transformer,cors,key-auth,acl,basic-auth
      KONG_NGINX_PROXY_PROXY_BUFFER_SIZE: 160k
      KONG_NGINX_PROXY_PROXY_BUFFERS: 64 160k
      # SUPABASE_ANON_KEY, SUPABASE_SERVICE_KEY should be set via Dokploy Environment variables
      # DASHBOARD_USERNAME, DASHBOARD_PASSWORD should be set via Dokploy Environment variables
    entrypoint: bash -c 'eval "echo \"$$(cat ~/temp.yml)\"" > ~/kong.yml && /docker-entrypoint.sh kong docker-start'

  auth:
    container_name: supabase-auth
    image: supabase/gotrue:v2.171.0
    restart: unless-stopped
    networks:
      - dokploy-network # Add to Dokploy network
    healthcheck:
      test:
        [
          "CMD",
          "wget",
          "--no-verbose",
          "--tries=1",
          "--spider",
          "http://localhost:9999/health"
        ]
      timeout: 5s
      interval: 5s
      retries: 3
    depends_on:
      db:
        condition: service_healthy
      analytics:
        condition: service_healthy
    environment:
      GOTRUE_API_HOST: 0.0.0.0
      GOTRUE_API_PORT: 9999
      # API_EXTERNAL_URL should be set via Dokploy Environment variables
      # GOTRUE_DB_DATABASE_URL needs POSTGRES_PASSWORD, POSTGRES_HOST, POSTGRES_PORT, POSTGRES_DB from Dokploy Environment variables
      # GOTRUE_SITE_URL, GOTRUE_URI_ALLOW_LIST, GOTRUE_DISABLE_SIGNUP should be set via Dokploy Environment variables
      GOTRUE_JWT_ADMIN_ROLES: service_role
      GOTRUE_JWT_AUD: authenticated
      GOTRUE_JWT_DEFAULT_GROUP_NAME: authenticated
      # GOTRUE_JWT_EXP, GOTRUE_JWT_SECRET should be set via Dokploy Environment variables
      # GOTRUE_EXTERNAL_EMAIL_ENABLED, GOTRUE_EXTERNAL_ANONYMOUS_USERS_ENABLED, GOTRUE_MAILER_AUTOCONFIRM should be set via Dokploy Environment variables
      # SMTP_ADMIN_EMAIL, SMTP_HOST, SMTP_PORT, SMTP_USER, SMTP_PASS, SMTP_SENDER_NAME should be set via Dokploy Environment variables
      # MAILER_URLPATHS_* should be set via Dokploy Environment variables
      # GOTRUE_EXTERNAL_PHONE_ENABLED, GOTRUE_SMS_AUTOCONFIRM should be set via Dokploy Environment variables
      # GOTRUE_HOOK_* are optional

  rest:
    container_name: supabase-rest
    image: postgrest/postgrest:v12.2.11
    restart: unless-stopped
    networks:
      - dokploy-network # Add to Dokploy network
    depends_on:
      db:
        condition: service_healthy
      analytics:
        condition: service_healthy
    environment:
      # PGRST_DB_URI needs POSTGRES_PASSWORD, POSTGRES_HOST, POSTGRES_PORT, POSTGRES_DB from Dokploy Environment variables
      # PGRST_DB_SCHEMAS should be set via Dokploy Environment variables
      PGRST_DB_ANON_ROLE: anon
      # PGRST_JWT_SECRET should be set via Dokploy Environment variables
      PGRST_DB_USE_LEGACY_GUCS: "false"
      # PGRST_APP_SETTINGS_JWT_SECRET, PGRST_APP_SETTINGS_JWT_EXP should be set via Dokploy Environment variables
    command:
      [
        "postgrest"
      ]

  realtime:
    container_name: realtime-dev.supabase-realtime
    image: supabase/realtime:v2.34.47
    restart: unless-stopped
    networks:
      - dokploy-network # Add to Dokploy network
    depends_on:
      db:
        condition: service_healthy
      analytics:
        condition: service_healthy
    healthcheck:
      test:
        [
          "CMD",
          "curl",
          "-sSfL",
          "--head",
          "-o",
          "/dev/null",
          "-H",
          "Authorization: Bearer ${ANON_KEY}", # ANON_KEY from Dokploy Env
          "http://localhost:4000/api/tenants/realtime-dev/health"
        ]
      timeout: 5s
      interval: 5s
      retries: 3
    environment:
      PORT: 4000
      # DB_HOST, DB_PORT, DB_USER, DB_PASSWORD, DB_NAME needs POSTGRES_* from Dokploy Environment variables
      DB_AFTER_CONNECT_QUERY: 'SET search_path TO _realtime'
      DB_ENC_KEY: supabaserealtime
      # API_JWT_SECRET, SECRET_KEY_BASE should be set via Dokploy Environment variables
      ERL_AFLAGS: -proto_dist inet_tcp
      DNS_NODES: "''"
      RLIMIT_NOFILE: "10000"
      APP_NAME: realtime
      SEED_SELF_HOST: true
      RUN_JANITOR: true

  storage:
    container_name: supabase-storage
    image: supabase/storage-api:v1.22.7
    restart: unless-stopped
    networks:
      - dokploy-network # Add to Dokploy network
    volumes:
      # Relative path './volumes' is correct
      - ./volumes/storage:/var/lib/storage:z
    healthcheck:
      test:
        [
          "CMD",
          "wget",
          "--no-verbose",
          "--tries=1",
          "--spider",
          "http://storage:5000/status"
        ]
      timeout: 5s
      interval: 5s
      retries: 3
    depends_on:
      db:
        condition: service_healthy
      rest:
        condition: service_started
      imgproxy:
        condition: service_started
    environment:
      # ANON_KEY, SERVICE_KEY should be set via Dokploy Environment variables
      POSTGREST_URL: http://rest:3000
      # PGRST_JWT_SECRET should be set via Dokploy Environment variables
      # DATABASE_URL needs POSTGRES_PASSWORD, POSTGRES_HOST, POSTGRES_PORT, POSTGRES_DB from Dokploy Environment variables
      FILE_SIZE_LIMIT: 52428800
      STORAGE_BACKEND: file
      FILE_STORAGE_BACKEND_PATH: /var/lib/storage
      TENANT_ID: stub # TODO: https://github.com/supabase/storage-api/issues/55 ? Maybe use POOLER_TENANT_ID? Check Supabase docs.
      REGION: stub
      GLOBAL_S3_BUCKET: stub
      ENABLE_IMAGE_TRANSFORMATION: "true"
      IMGPROXY_URL: http://imgproxy:5001

  imgproxy:
    container_name: supabase-imgproxy
    image: darthsim/imgproxy:v3.8.0
    restart: unless-stopped
    networks:
      - dokploy-network # Add to Dokploy network
    volumes:
      # Relative path './volumes' is correct
      - ./volumes/storage:/var/lib/storage:z
    healthcheck:
      test:
        [
          "CMD",
          "imgproxy",
          "health"
        ]
      timeout: 5s
      interval: 5s
      retries: 3
    environment:
      IMGPROXY_BIND: ":5001"
      IMGPROXY_LOCAL_FILESYSTEM_ROOT: /
      IMGPROXY_USE_ETAG: "true"
      # IMGPROXY_ENABLE_WEBP_DETECTION should be set via Dokploy Environment variables

  meta:
    container_name: supabase-meta
    image: supabase/postgres-meta:v0.88.9
    restart: unless-stopped
    networks:
      - dokploy-network # Add to Dokploy network
    depends_on:
      db:
        condition: service_healthy
      analytics:
        condition: service_healthy
    environment:
      PG_META_PORT: 8080
      # PG_META_DB_HOST, PG_META_DB_PORT, PG_META_DB_NAME, PG_META_DB_USER, PG_META_DB_PASSWORD needs POSTGRES_* and POSTGRES_PASSWORD from Dokploy Environment variables

  functions:
    container_name: supabase-edge-functions
    image: supabase/edge-runtime:v1.67.4
    restart: unless-stopped
    networks:
      - dokploy-network # Add to Dokploy network
    volumes:
      # Relative path './volumes' is correct
      - ./volumes/functions:/home/deno/functions:Z
    depends_on:
      analytics:
        condition: service_healthy
    environment:
      # JWT_SECRET should be set via Dokploy Environment variables
      SUPABASE_URL: http://kong:8000
      # SUPABASE_ANON_KEY, SUPABASE_SERVICE_ROLE_KEY, SUPABASE_DB_URL needs POSTGRES_PASSWORD, POSTGRES_HOST, POSTGRES_PORT, POSTGRES_DB from Dokploy Environment variables
      # VERIFY_JWT should be set via Dokploy Environment variables
    command:
      [
        "start",
        "--main-service",
        "/home/deno/functions/main"
      ]

  analytics:
    container_name: supabase-analytics
    image: supabase/logflare:1.12.0
    restart: unless-stopped
    networks:
      - dokploy-network # Add to Dokploy network
    # Expose port 4000 internally for other services and potentially Dokploy routing/health checks
    expose:
      - 4000
    # Comment to use Big Query backend for analytics
    # volumes:
    #   - type: bind
    #     source: ${PWD}/gcloud.json # This path might need adjustment depending on Dokploy's staging
    #     target: /opt/app/rel/logflare/bin/gcloud.json
    #     read_only: true
    healthcheck:
      test:
        [
          "CMD",
          "curl",
          "http://localhost:4000/health"
        ]
      timeout: 5s
      interval: 5s
      retries: 10
    depends_on:
      db:
        condition: service_healthy
    environment:
      LOGFLARE_NODE_HOST: 127.0.0.1
      # DB_USERNAME, DB_DATABASE, DB_HOSTNAME, DB_PORT, DB_PASSWORD, DB_SCHEMA needs POSTGRES_* from Dokploy Environment variables
      # LOGFLARE_API_KEY should be set via Dokploy Environment variables
      LOGFLARE_SINGLE_TENANT: true
      LOGFLARE_SUPABASE_MODE: true
      LOGFLARE_MIN_CLUSTER_SIZE: 1
      # POSTGRES_BACKEND_URL needs POSTGRES_PASSWORD, POSTGRES_HOST, POSTGRES_PORT from Dokploy Environment variables
      POSTGRES_BACKEND_SCHEMA: _analytics
      LOGFLARE_FEATURE_FLAG_OVERRIDE: multibackend=true
      # Google Cloud vars commented out as per default setup

  # Comment out everything below this point if you are using an external Postgres database
  db:
    container_name: supabase-db
    image: supabase/postgres:15.8.1.060
    restart: unless-stopped
    networks:
      - dokploy-network # Add to Dokploy network
    volumes:
      # Relative paths './volumes' are correct
      - ./volumes/db/realtime.sql:/docker-entrypoint-initdb.d/migrations/99-realtime.sql:Z
      - ./volumes/db/webhooks.sql:/docker-entrypoint-initdb.d/init-scripts/98-webhooks.sql:Z
      - ./volumes/db/roles.sql:/docker-entrypoint-initdb.d/init-scripts/99-roles.sql:Z
      - ./volumes/db/jwt.sql:/docker-entrypoint-initdb.d/init-scripts/99-jwt.sql:Z
      - ./volumes/db/data:/var/lib/postgresql/data:Z # Persistent data volume
      - ./volumes/db/_supabase.sql:/docker-entrypoint-initdb.d/migrations/97-_supabase.sql:Z
      - ./volumes/db/logs.sql:/docker-entrypoint-initdb.d/migrations/99-logs.sql:Z
      - ./volumes/db/pooler.sql:/docker-entrypoint-initdb.d/migrations/99-pooler.sql:Z
      # Use named volume to persist pgsodium decryption key between restarts
      - db-config:/etc/postgresql-custom
    healthcheck:
      test:
        [
        "CMD",
        "pg_isready",
        "-U",
        "postgres",
        "-h",
        "localhost"
        ]
      interval: 5s
      timeout: 5s
      retries: 10
    depends_on:
      vector:
        condition: service_healthy
    environment:
      POSTGRES_HOST: /var/run/postgresql # This is for pg_isready check within the container
      PGPORT: 5432 # Hardcode or use env var? Let's use env var for consistency
      # POSTGRES_PORT should be set via Dokploy Environment variables
      # PGPASSWORD, POSTGRES_PASSWORD should be set via Dokploy Environment variables
      # PGDATABASE, POSTGRES_DB should be set via Dokploy Environment variables
      # JWT_SECRET, JWT_EXP should be set via Dokploy Environment variables
      # DB_HOST: /var/run/postgresql # Add if other services need to connect directly using service name 'db' instead of relying on pooler
    command:
      [
        "postgres",
        "-c",
        "config_file=/etc/postgresql/postgresql.conf",
        "-c",
        "log_min_messages=fatal"
      ]

  vector:
    container_name: supabase-vector
    image: timberio/vector:0.28.1-alpine
    restart: unless-stopped
    networks:
      - dokploy-network # Add to Dokploy network
    volumes:
      # Relative path './volumes' is correct
      - ./volumes/logs/vector.yml:/etc/vector/vector.yml:ro,z
      # This binds the Docker socket. Ensure this path is correct on your VPS and accessible to Dokploy
      - ${DOCKER_SOCKET_LOCATION}:/var/run/docker.sock:ro,z
    healthcheck:
      test:
        [
          "CMD",
          "wget",
          "--no-verbose",
          "--tries=1",
          "--spider",
          "http://vector:9001/health"
        ]
      timeout: 5s
      interval: 5s
      retries: 3
    environment:
      # LOGFLARE_API_KEY should be set via Dokploy Environment variables
    command:
      [
        "--config",
        "/etc/vector/vector.yml"
      ]
    security_opt:
      - "label=disable" # Might be needed depending on Dokploy's security context

  supavisor:
    container_name: supabase-pooler
    image: supabase/supavisor:2.5.1
    restart: unless-stopped
    networks:
      - dokploy-network # Add to Dokploy network
    # Map host ports for the pooler as in your old Dokploy compose and the official compose
    ports:
      - ${POSTGRES_PORT}:5432 # Standard DB port mapped to Supavisor
      - ${POOLER_PROXY_PORT_TRANSACTION}:6543 # Transaction pool mode port
    volumes:
      # Relative path './volumes' is correct
      - ./volumes/pooler/pooler.exs:/etc/pooler/pooler.exs:ro,z
    healthcheck:
      test:
        [
          "CMD",
          "curl",
          "-sSfL",
          "--head",
          "-o",
          "/dev/null",
          "http://127.0.0.1:4000/api/health"
        ]
      interval: 10s
      timeout: 5s
      retries: 5
    depends_on:
      db:
        condition: service_healthy
      analytics:
        condition: service_healthy
    environment:
      PORT: 4000
      # POSTGRES_PORT, POSTGRES_DB, POSTGRES_PASSWORD should be set via Dokploy Environment variables
      DATABASE_URL: ecto://supabase_admin:${POSTGRES_PASSWORD}@db:${POSTGRES_PORT}/_supabase # Connects to the 'db' service
      CLUSTER_POSTGRES: true
      # SECRET_KEY_BASE, VAULT_ENC_KEY should be set via Dokploy Environment variables
      # API_JWT_SECRET, METRICS_JWT_SECRET needs JWT_SECRET from Dokploy Env
      REGION: local
      ERL_AFLAGS: -proto_dist inet_tcp
      # POOLER_TENANT_ID, POOLER_DEFAULT_POOL_SIZE, POOLER_MAX_CLIENT_CONN should be set via Dokploy Environment variables
      POOLER_POOL_MODE: transaction
    command:
      [
        "/bin/sh",
        "-c",
        "/app/bin/migrate && /app/bin/supavisor eval \"$$(cat /etc/pooler/pooler.exs)\" && /app/bin/server"
      ]

volumes:
  db-config:

networks:
  dokploy-network:
    external: true
